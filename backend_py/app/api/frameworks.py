from fastapi import (
    APIRouter,
    UploadFile,
    File,
    HTTPException,
    BackgroundTasks,
    Depends,
    Form,
    Query,
)
from fastapi.responses import Response
from pydantic import BaseModel
from typing import Optional, List
from sqlalchemy.orm import Session
import json
import tempfile
import os
import random
from pathlib import Path
from datetime import datetime
from nanoid import generate

# Database
from ..db import get_db
from ..models import Framework, FRAMEWORK_GROUPS
from ..auth import get_current_user_id

# LLM
import sys

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from llm_local import extract_seed
    from llm_global import (
        build_mock_framework,
        call_openai_framework,
        resolve_api_settings,
    )
except ImportError as e:
    print(f"Warning: Could not import LLM modules: {e}")
    print("Make sure llm_local.py and llm_global.py are in the correct location")


router = APIRouter(prefix="/api/frameworks", tags=["frameworks"])


# ============= Request/Response Models =============


class TextGenerateRequest(BaseModel):
    text: str
    use_global_llm: bool = True
    model: str = "gpt-4o"
    user_id: Optional[str] = None


class GenerateResponse(BaseModel):
    success: bool
    framework_id: Optional[str] = None
    framework: Optional[dict] = None
    frameworks: Optional[List[dict]] = None  # å¤šä¸ªframework
    # Local LLM
    metadata: Optional[dict] = None
    error: Optional[str] = None


class RegenerateRequest(BaseModel):
    framework: dict
    use_local: bool = False


class FrameworkListResponse(BaseModel):
    """æ¡†æ¶åˆ—è¡¨å“åº”"""

    id: str
    title: str
    version: str
    family: str
    confidence: float
    created_at: datetime
    updated_at: datetime

    # ç®€åŒ–çš„å†…å®¹é¢„è§ˆï¼ˆç”¨äºå¡ç‰‡æ˜¾ç¤ºï¼‰
    preview_artefacts: List[dict]  # æœ€å¤š3ä¸ªartefact


class FrameworkDetailResponse(BaseModel):
    """æ¡†æ¶è¯¦æƒ…å“åº”"""

    id: str
    title: str
    version: str
    family: str
    confidence: float
    creator_id: str
    metadata: dict
    steps: List[dict]
    artefacts: dict
    risks: List[dict]
    escalation: List[dict]
    created_at: datetime
    updated_at: datetime


# ============= Helper Functions =============


def calculate_mock_confidence() -> float:
    """
    ç”Ÿæˆ mock confidence åˆ†æ•° (60-95)
    æœªæ¥å¯ä»¥åŸºäº AI è®¡ç®—çœŸå®çš„ç½®ä¿¡åº¦
    """
    return round(random.uniform(60, 95), 1)


def ensure_family_in_framework(framework: dict) -> str:
    family = framework.get("family") or framework.get("category")

    if family and family in FRAMEWORK_GROUPS:
        return family

    # å¦‚æœ AI æ²¡è¿”å›æˆ–è¿”å›äº†æ— æ•ˆå€¼ï¼Œå°è¯•åŸºäº title æ™ºèƒ½æ¨æ–­
    title = framework.get("title", "").lower()
    # æ‰©å±•å…³é”®è¯åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    # Technology & AI
    if any(
        word in title
        for word in [
            "ai",
            "artificial intelligence",
            "machine learning",
            "ml",
            "tech",
            "software",
            "system",
            "platform",
            "data",
            "algorithm",
            "digital",
            "cloud",
            "api",
            "code",
            "programming",
        ]
    ):
        return "Technology"

    # Healthcare & Wellbeing  âœ… æ·»åŠ  wellbeing, wellness
    elif any(
        word in title
        for word in [
            "health",
            "medical",
            "patient",
            "hospital",
            "clinical",
            "wellbeing",
            "wellness",
            "healthcare",
            "care",
            "medicine",
            "diagnosis",
            "treatment",
            "therapy",
            "pharmaceutical",
        ]
    ):
        return "Healthcare"

    # Legal & Compliance
    elif any(
        word in title
        for word in [
            "legal",
            "law",
            "compliance",
            "regulation",
            "regulatory",
            "audit",
            "governance",
            "policy",
            "risk management",
            "gdpr",
            "privacy",
            "data protection",
            "contract",
        ]
    ):
        return "Legal"

    # Financial
    elif any(
        word in title
        for word in [
            "finance",
            "financial",
            "bank",
            "invest",
            "investment",
            "accounting",
            "treasury",
            "payment",
            "trading",
            "fund",
            "capital",
            "credit",
            "loan",
            "insurance",
        ]
    ):
        return "Financial"

    # Education & Training
    elif any(
        word in title
        for word in [
            "education",
            "training",
            "learning",
            "course",
            "curriculum",
            "teaching",
            "student",
            "academic",
            "school",
            "university",
            "certification",
            "workshop",
        ]
    ):
        return "Education"

    # Marketing & Brand
    elif any(
        word in title
        for word in [
            "marketing",
            "brand",
            "campaign",
            "advertising",
            "promotion",
            "social media",
            "seo",
            "content marketing",
            "pr",
            "communication",
            "outreach",
        ]
    ):
        return "Marketing"

    # Operations & Process
    elif any(
        word in title
        for word in [
            "operation",
            "process",
            "workflow",
            "supply chain",
            "logistics",
            "manufacturing",
            "production",
            "delivery",
            "optimization",
            "efficiency",
        ]
    ):
        return "Operations"

    # Human Resources
    elif any(
        word in title
        for word in [
            "hr",
            "human resource",
            "recruit",
            "employee",
            "talent",
            "hiring",
            "onboarding",
            "performance",
            "compensation",
            "benefits",
            "workforce",
        ]
    ):
        return "Human Resources"

    # Sales & Business Development
    elif any(
        word in title
        for word in [
            "sales",
            "sell",
            "selling",
            "revenue",
            "customer",
            "business development",
            "account management",
            "crm",
            "pipeline",
            "deal",
        ]
    ):
        return "Sales"

    # Design & UX
    elif any(
        word in title
        for word in [
            "design",
            "ux",
            "ui",
            "user experience",
            "interface",
            "product design",
            "visual",
            "creative",
            "prototype",
            "wireframe",
            "mockup",
        ]
    ):
        return "Design"

    # Research & Analysis
    elif any(
        word in title
        for word in [
            "research",
            "study",
            "analysis",
            "investigation",
            "survey",
            "questionnaire",
            "data collection",
            "findings",
            "methodology",
            "hypothesis",
        ]
    ):
        return "Research"

    # Strategy & Planning
    elif any(
        word in title
        for word in [
            "strategy",
            "strategic",
            "planning",
            "roadmap",
            "business plan",
            "vision",
            "mission",
            "objectives",
            "goals",
            "initiative",
        ]
    ):
        return "Strategy"

    # Project Management
    elif any(
        word in title
        for word in [
            "project",
            "program",
            "delivery",
            "implementation",
            "milestone",
            "sprint",
            "agile",
            "scrum",
            "waterfall",
            "gantt",
            "timeline",
        ]
    ):
        return "Project Management"

    # Default fallback
    else:
        return "Other"


def process_with_local_llm(input_data: str, is_file: bool = False) -> dict:
    """
    æ­¥éª¤ 1: ä½¿ç”¨ Local LLM (Cloud or Ollama) æå–å…ƒæ•°æ®

    ç°åœ¨æ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼š
    - LLM_TYPE: "cloud" æˆ– "local"
    - LOCAL_LLM_URL: Cloud LLMåœ°å€ (ä¾‹å¦‚: http://34.87.13.228:8000/v1)

    Args:
        input_data: æ–‡ä»¶è·¯å¾„æˆ–æ–‡æœ¬å†…å®¹
        is_file: æ˜¯å¦ä¸ºæ–‡ä»¶è·¯å¾„

    Returns:
        metadata: æå–çš„ç»“æ„åŒ–å…ƒæ•°æ®
    """
    try:
        # âœ… ä¸å†ç¡¬ç¼–ç  host å’Œ modelï¼Œè®© extract_seed ä»ç¯å¢ƒå˜é‡è¯»å–
        # è¿™æ ·å°±èƒ½æ­£ç¡®ä½¿ç”¨ Cloud LLM è€Œä¸æ˜¯æœ¬åœ° Ollama
        print(
            f"ğŸ”„ Step 1: Processing {'file' if is_file else 'text'} with Local LLM (Privacy Protection)..."
        )

        metadata = extract_seed(input_data=input_data)

        return metadata

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Local LLM processing failed: {str(e)}"
        )


def process_with_global_llm(
    metadata: dict, model: str = "gpt-4o", use_mock: bool = False
) -> dict:
    """
    æ­¥éª¤ 2: ä½¿ç”¨ Global LLM (OpenAI) ç”Ÿæˆæ¡†æ¶

    æ³¨æ„ï¼šè¿™é‡Œä¼šè®© AI è‡ªåŠ¨åˆ†é… family å­—æ®µ
    """
    try:
        api_key, base_url = resolve_api_settings(None, None)

        if use_mock or not api_key:
            print("â„¹ï¸  Using mock framework generation (no OpenAI API key)")
            framework = build_mock_framework(metadata)
        else:
            print(f"ğŸŒ Calling OpenAI API with model: {model}")

            # ğŸ”¥ å¢å¼º promptï¼Œè®© AI åˆ†é… family
            # æ³¨æ„ï¼šè¿™éœ€è¦ä¿®æ”¹ llm_global.py ä¸­çš„ prompt
            # æˆ–è€…åœ¨è¿™é‡Œæ·»åŠ é¢å¤–çš„ API è°ƒç”¨æ¥åˆ†ç±»

            framework = call_openai_framework(
                md=metadata,
                model=model,
                timeout=180,
                api_key=api_key,
                base_url=base_url,
                verbose=True,
            )
            print("âœ… OpenAI API call successful")

        # ç¡®ä¿ family å­—æ®µå­˜åœ¨
        # framework['family'] = ensure_family_in_framework(framework)

        return framework

    except Exception as e:
        import traceback

        print("âŒ Global LLM Error:")
        print(traceback.format_exc())

        raise HTTPException(
            status_code=500, detail=f"Global LLM processing failed: {str(e)}"
        )


def save_framework_to_db(
    framework_data: dict, metadata_dict: dict, creator_id: str, db: Session
) -> Framework:
    """
    å°†ç”Ÿæˆçš„ framework ä¿å­˜åˆ°æ•°æ®åº“

    Args:
        framework_data: AI ç”Ÿæˆçš„å®Œæ•´æ¡†æ¶
        metadata_dict: Local LLM æå–çš„ metadata
        creator_id: åˆ›å»ºè€…ç”¨æˆ· ID
        db: æ•°æ®åº“ session

    Returns:
        ä¿å­˜çš„ Framework å¯¹è±¡
    """

    # ç”Ÿæˆæ¡†æ¶ ID
    framework_id = f"fw_{generate(size=12)}"

    # æå–å„éƒ¨åˆ†æ•°æ®
    metadata = framework_data.get("metadata", {})
    steps = framework_data.get("steps", [])
    artefacts = framework_data.get("artefacts", {})
    risks = framework_data.get("risks", [])
    escalation = framework_data.get("escalation", [])
    pov = framework_data.get("pov")
    family = framework_data.get("family", "Other")
    confidence = float(framework_data.get("confidence", 0))

    # è·å–åŸºæœ¬ä¿¡æ¯
    title = metadata.get("title") or framework_data.get("title", "Untitled Framework")
    version = metadata.get("version", "1.0.0")
    # family = ensure_family_in_framework(framework_data)
    # confidence = calculate_mock_confidence()

    # new
    family = framework_data.get("family", "Other")
    confidence = float(framework_data.get("confidence", 0))
    pov = framework_data.get("pov", None)

    # åˆ›å»ºæ•°æ®åº“è®°å½•
    db_framework = Framework(
        id=framework_id,
        title=title,
        version=version,
        creator_id=creator_id,
        metadata_json=json.dumps(metadata, ensure_ascii=False),
        steps_json=json.dumps(steps, ensure_ascii=False),
        artefacts_json=json.dumps(artefacts, ensure_ascii=False),
        risks_json=json.dumps(risks, ensure_ascii=False),
        escalation_json=json.dumps(escalation, ensure_ascii=False),
        raw_framework_json=json.dumps(framework_data, ensure_ascii=False),
        raw_metadata_json=json.dumps(metadata_dict, ensure_ascii=False),
        created_at=datetime.utcnow(),
        updated_at=datetime.utcnow(),
        pov=pov,
        family=family,
        confidence=confidence,
    )

    db.add(db_framework)
    db.commit()
    db.refresh(db_framework)

    return db_framework


# ============= API Endpoints =============


@router.post("/generate-from-text", response_model=GenerateResponse)
async def generate_from_text(
    request: TextGenerateRequest,
    db: Session = Depends(get_db),
):
    user_id = getattr(request, "user_id", None)
    """
    ä»æ–‡æœ¬ç”Ÿæˆæ¡†æ¶ï¼ˆéœ€è¦ç™»å½•ï¼‰

    è°ƒç”¨é“¾è·¯: å‰ç«¯æ–‡æœ¬ â†’ æœ¬åœ° LLM â†’ Global LLM â†’ ä¿å­˜æ•°æ®åº“ â†’ è¿”å›æ¡†æ¶
    """
    try:
        if not request.text.strip():
            raise HTTPException(status_code=400, detail="Text content is empty")

        if len(request.text) > 50000:
            raise HTTPException(
                status_code=400, detail="Text too long (max 50,000 characters)"
            )

        # âœ… æ ¹æ® use_global_llm å†³å®šæ˜¯å¦ä½¿ç”¨ Local LLM
        if not request.use_global_llm:
            # ğŸ”’ Lock ON: éšç§ä¿æŠ¤æ¨¡å¼
            print("ğŸ”„ Step 1: Processing with Local LLM (Privacy Protection)...")
            metadata = process_with_local_llm(request.text, is_file=False)
            print(f"âœ… Local LLM completed. Extracted {len(metadata)} metadata fields")

            print("ğŸ”„ Step 2: Processing with Global LLM...")
            framework_result = process_with_global_llm(
                metadata=metadata, model=request.model, use_mock=False
            )
            print("âœ… Global LLM completed")
        else:
            # ğŸ”“ Lock OFF: å¿«é€Ÿæ¨¡å¼
            print("ğŸ”„ Processing with Global LLM (Fast Mode - No Local Processing)...")

            # ğŸ”‘ 1. æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œæˆ–å‰150å­—ç¬¦ï¼‰
            lines = request.text.strip().split("\n")
            potential_title = lines[0][:150].strip() if lines else "User Content"

            # ğŸ”‘ 2. ç®€å•å…³é”®è¯æå–ï¼ˆä»æ ‡é¢˜ä¸­æå–ï¼‰
            # å–é•¿åº¦>3çš„å•è¯ï¼Œæœ€å¤š5ä¸ª
            simple_keywords = [
                word.strip()
                for word in potential_title.lower().split()
                if len(word.strip()) > 3
            ][:5]

            # ğŸ”‘ 3. æå–ç« èŠ‚ç»“æ„ï¼ˆå‰5ä¸ªæ®µè½æˆ–ç« èŠ‚ï¼‰
            # ä¸ä¼ å®Œæ•´å†…å®¹ï¼Œåªä¼ æ ‡é¢˜
            sections = []
            current_section_lines = []

            for line in lines[:100]:  # åªçœ‹å‰100è¡Œ
                line_stripped = line.strip()
                if not line_stripped:
                    continue

                # åˆ¤æ–­æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜ï¼ˆç®€å•è§„åˆ™ï¼šè¾ƒçŸ­çš„è¡Œï¼Œæˆ–åŒ…å«æ•°å­—ï¼‰
                if len(line_stripped) < 100 and (
                    line_stripped[0].isdigit()
                    or line_stripped.isupper()
                    or any(
                        marker in line_stripped.lower()
                        for marker in ["step", "phase", "stage", "chapter"]
                    )
                ):
                    if current_section_lines:
                        # ä¿å­˜å‰ä¸€ä¸ªsectionï¼ˆåªä¿ç•™å‰200å­—ä½œä¸ºæ‘˜è¦ï¼‰
                        content_preview = " ".join(current_section_lines)[:200]
                        sections.append(
                            {
                                "title": current_section_lines[0][:150],
                                "content": content_preview,  # âœ… åªä¿ç•™å‰200å­—
                                "level": 1,
                            }
                        )
                        current_section_lines = [line_stripped]
                    else:
                        current_section_lines = [line_stripped]
                else:
                    if len(current_section_lines) < 3:  # æ¯ä¸ªsectionæœ€å¤šä¿ç•™3è¡Œé¢„è§ˆ
                        current_section_lines.append(line_stripped)

            # ä¿å­˜æœ€åä¸€ä¸ªsection
            if current_section_lines:
                content_preview = " ".join(current_section_lines)[:200]
                sections.append(
                    {
                        "title": current_section_lines[0][:150],
                        "content": content_preview,
                        "level": 1,
                    }
                )

            # å¦‚æœæ²¡æœ‰æå–åˆ°sectionsï¼Œä½¿ç”¨ç®€å•çš„åˆ†æ®µ
            if not sections:
                # ç®€å•åˆ†æ®µï¼šæ¯500å­—ä¸€ä¸ªsection
                text_parts = [
                    request.text[i : i + 500]
                    for i in range(0, min(len(request.text), 2500), 500)
                ]
                sections = [
                    {
                        "title": f"Section {i+1}",
                        "content": part[:200] + "...",  # âœ… æ¯ä¸ªsectionåªä¿ç•™å‰200å­—
                        "level": 1,
                    }
                    for i, part in enumerate(text_parts)
                ]

            # âœ… 4. åˆ›å»ºä¼˜åŒ–çš„ metadataï¼ˆå‚è€ƒLock ONçš„ç»“æ„ï¼‰
            metadata = {
                "doc_id": f"doc-{generate(size=12)}",
                "title": potential_title,  # âœ… çœŸå®æ ‡é¢˜
                "subject": potential_title,
                "language": "en",
                "bypass_local_llm": True,
                # âœ… å…³é”®å­—æ®µ
                "keywords": simple_keywords,  # âœ… 5-10ä¸ªå…³é”®è¯
                # âœ… sectionsï¼šåªåŒ…å«ç« èŠ‚æ ‡é¢˜å’Œå‰200å­—é¢„è§ˆ
                "sections": sections[:10],  # æœ€å¤š10ä¸ªsections
                # âœ… facetsï¼šç®€å•çš„ä¸»é¢˜åˆ†ç±»
                "facets": {
                    "main_topic": {
                        "summary": potential_title,
                        "items": [
                            {
                                "value": kw,
                                "evidence": "",
                                "location": "",
                                "confidence": 0.8,
                            }
                            for kw in simple_keywords
                        ],
                    }
                },
                # âœ… key_valuesï¼šå…³é”®ä¿¡æ¯é”®å€¼å¯¹
                "key_values": [
                    {"key": "document_title", "value": potential_title},
                    {"key": "processing_mode", "value": "direct"},
                    {"key": "section_count", "value": str(len(sections))},
                ],
                # âœ… tagsï¼šä½¿ç”¨å…³é”®è¯
                "tags": simple_keywords,
                # å…¶ä»–å¿…éœ€å­—æ®µï¼ˆä¿æŒä¸ºç©ºï¼‰
                "triples": [],
                "questions": [],
                "risks": [],
                "actions_todo": [],
                "metrics": [],
                "tables": [],
                "figures": [],
                "extra": {
                    "processing_mode": "direct",
                    "note": "Extracted structure without full text to reduce prompt size",
                    "original_length": len(request.text),
                    "truncated": True,
                },
            }

            # ä¸æ·»åŠ  raw_text æˆ– full_contentï¼
            # ChatGPTä¸éœ€è¦å®Œæ•´åŸæ–‡ï¼Œåªéœ€è¦ç»“æ„åŒ–ä¿¡æ¯

            framework_result = process_with_global_llm(
                metadata=metadata, model=request.model, use_mock=False
            )
            print("âœ… Global LLM completed")

        # ğŸ”§ ä¿®æ”¹ï¼šæ”¯æŒå¤š POV / å¤š framework ç»“æœ
        # âœ… æ”¯æŒå¤š POV / å¤š framework ç»“æœ
        frameworks = framework_result.get("frameworks", [framework_result])

        print(f"âœ… Framework generation completed: {len(frameworks)} framework(s)")

        # âœ… ç›´æ¥è¿”å›ç”Ÿæˆçš„æ•°æ®ï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆç”±å‰ç«¯ä¿å­˜åˆ° Firebaseï¼‰
        return GenerateResponse(
            success=True,
            framework_id=None,  # å‰ç«¯åˆ›å»ºåä¼šæœ‰ ID
            framework=frameworks[0] if frameworks else None,
            frameworks=frameworks,
            metadata=metadata,
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error in generate_from_text: {str(e)}")
        import traceback

        traceback.print_exc()
        return GenerateResponse(success=False, error=str(e))


@router.post("/generate-from-file", response_model=GenerateResponse)
async def generate_from_file(
    file: UploadFile = File(...),
    use_global_llm: bool = True,
    model: str = "gpt-4o",
    user_id: str = Depends(get_current_user_id),
    db: Session = Depends(get_db),
):
    """
    ä»ä¸Šä¼ æ–‡ä»¶ç”Ÿæˆæ¡†æ¶ï¼ˆéœ€è¦ç™»å½•ï¼‰

    è°ƒç”¨é“¾è·¯: å‰ç«¯æ–‡ä»¶ â†’ æœ¬åœ° LLM â†’ Global LLM â†’ ä¿å­˜æ•°æ®åº“ â†’ è¿”å›æ¡†æ¶
    """
    temp_path = None

    try:
        # éªŒè¯æ–‡ä»¶
        if not file.filename:
            raise HTTPException(status_code=400, detail="No file provided")

        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        allowed_extensions = {".txt", ".pdf", ".doc", ".docx", ".md"}
        file_ext = Path(file.filename).suffix.lower()

        if file_ext not in allowed_extensions:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported file type. Allowed: {', '.join(allowed_extensions)}",
            )

        # æ£€æŸ¥æ–‡ä»¶å¤§å° (10MB)
        content = await file.read()
        if len(content) > 10 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="File too large (max 10MB)")

        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
            tmp.write(content)
            temp_path = tmp.name

        print(f"ğŸ“ File saved to: {temp_path}")

        # æ­¥éª¤ 1: æœ¬åœ° LLM æå–å…ƒæ•°æ®
        print("ğŸ”„ Step 1: Processing with Local LLM (Ollama)...")
        metadata = process_with_local_llm(temp_path, is_file=True)
        print(f"âœ… Local LLM completed. Extracted {len(metadata)} metadata fields")

        # æ­¥éª¤ 2: Global LLM ç”Ÿæˆæ¡†æ¶
        print("ğŸ”„ Step 2: Processing with Global LLM (OpenAI)...")
        framework_result = process_with_global_llm(  # âœ… MODIFIED
            metadata=metadata, model=model, use_mock=not use_global_llm
        )
        print("âœ… Global LLM completed. Framework generated")

        # âœ… MODIFIED: æ”¯æŒå¤š POV è¾“å‡º
        frameworks = framework_result.get("frameworks", [framework_result])

        # ğŸ”¥ æ­¥éª¤ 3: ä¿å­˜åˆ°æ•°æ®åº“
        print("ğŸ’¾ Step 3: Saving framework(s) to database...")
        saved_ids = []  # âœ… MODIFIED
        for fw_data in frameworks:  # âœ… MODIFIED
            db_framework = save_framework_to_db(  # âœ… MODIFIED
                framework_data=fw_data,  # âœ… MODIFIED
                metadata_dict=metadata,
                creator_id=user_id,
                db=db,
            )
            saved_ids.append(db_framework.id)  # âœ… MODIFIED
        print(f"âœ… All frameworks saved: {len(saved_ids)} total")  # âœ… MODIFIED

        # âœ… MODIFIED: åŒæ—¶è¿”å›å•ä¸ªä¸å¤šä¸ªï¼ˆå‘åå…¼å®¹ï¼‰
        return GenerateResponse(
            success=True,
            framework_id=saved_ids[0] if saved_ids else None,  # âœ… MODIFIED
            framework=frameworks[0] if frameworks else None,  # âœ… MODIFIED
            frameworks=frameworks,  # âœ… MODIFIED
            metadata=metadata,
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error in generate_from_file: {str(e)}")
        import traceback

        traceback.print_exc()
        return GenerateResponse(success=False, error=str(e))
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_path and os.path.exists(temp_path):
            try:
                os.unlink(temp_path)
            except:
                pass


@router.post("/generate-from-files", response_model=GenerateResponse)
async def generate_from_files(
    files: List[UploadFile] = File(...),
    use_global_llm: bool = True,
    model: str = "gpt-4o",
    # No need for this anymore, change to firebase
    ## user_id: str = Depends(get_current_user_id),
    user_id: str = Form(None),
    db: Session = Depends(get_db),
):
    """
    ä»å¤šä¸ªæ–‡ä»¶ç”Ÿæˆæ¡†æ¶ï¼ˆéœ€è¦ç™»å½•ï¼‰

    å¤šä¸ªæ–‡ä»¶ä¼šè¢«åˆå¹¶å¤„ç†
    """
    temp_paths = []

    try:
        if not files or len(files) == 0:
            raise HTTPException(status_code=400, detail="No files provided")

        if len(files) > 10:
            raise HTTPException(status_code=400, detail="Too many files (max 10)")

        # ä¿å­˜æ‰€æœ‰æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        for file in files:
            if not file.filename:
                continue

            file_ext = Path(file.filename).suffix.lower()
            allowed_extensions = {".txt", ".pdf", ".doc", ".docx", ".md"}

            if file_ext not in allowed_extensions:
                continue

            content = await file.read()
            if len(content) > 10 * 1024 * 1024:
                raise HTTPException(
                    status_code=400, detail=f"File {file.filename} too large"
                )

            with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
                tmp.write(content)
                temp_paths.append(tmp.name)

        if not temp_paths:
            raise HTTPException(status_code=400, detail="No valid files")

        print(f"ğŸ“ Saved {len(temp_paths)} files")

        # âœ… æ ¹æ® use_global_llm å†³å®šæ˜¯å¦ä½¿ç”¨ Local LLM
        if not use_global_llm:
            # ğŸ”’ Lock ON: éšç§ä¿æŠ¤æ¨¡å¼
            print("ğŸ”„ Step 1: Processing files with Local LLM (Privacy Protection)...")
            all_metadata = []

            for temp_path in temp_paths:
                metadata = process_with_local_llm(temp_path, is_file=True)
                all_metadata.append(metadata)

            merged_metadata = all_metadata[0] if all_metadata else {}
            if len(all_metadata) > 1:
                merged_metadata["source_count"] = len(all_metadata)
                merged_metadata["merged_from_multiple_files"] = True

            print(f"âœ… Local LLM completed. Processed {len(temp_paths)} files")

            print("ğŸ”„ Step 2: Processing with Global LLM...")
            framework_result = process_with_global_llm(
                metadata=merged_metadata, model=model, use_mock=False
            )
            print("âœ… Global LLM completed")
        else:
            # ğŸ”“ Lock OFF: å¿«é€Ÿæ¨¡å¼
            print("ğŸ”„ Processing with Global LLM (Fast Mode - No Local Processing)...")

            # è¯»å–æ‰€æœ‰æ–‡ä»¶å†…å®¹
            file_contents = []
            file_names = []
            for i, temp_path in enumerate(temp_paths):
                try:
                    # è·å–æ–‡ä»¶å
                    original_filename = (
                        files[i].filename if i < len(files) else f"file_{i+1}"
                    )
                    file_names.append(original_filename)

                    for encoding in ["utf-8", "gbk", "latin-1"]:
                        try:
                            with open(temp_path, "r", encoding=encoding) as f:
                                content = f.read()
                                file_contents.append(content)
                                break
                        except (UnicodeDecodeError, UnicodeError):
                            continue
                except Exception as e:
                    print(f"Warning: Could not read file {temp_path}: {e}")

            # ğŸ”‘ 1. æ™ºèƒ½æå–æ ‡é¢˜
            if len(file_contents) == 1:
                # å•æ–‡ä»¶ï¼šä½¿ç”¨ç¬¬ä¸€è¡Œæˆ–æ–‡ä»¶å
                lines = file_contents[0].strip().split("\n")
                potential_title = (
                    lines[0][:150].strip()
                    if lines and len(lines[0].strip()) > 10
                    else file_names[0]
                )
            else:
                # å¤šæ–‡ä»¶ï¼šä½¿ç”¨ç»„åˆæè¿°
                lines = file_contents[0].strip().split("\n") if file_contents else []
                if lines and len(lines[0].strip()) > 10:
                    potential_title = lines[0][:150].strip()
                else:
                    potential_title = f"Framework from {len(file_names)} files"

            # ğŸ”‘ 2. ç®€å•å…³é”®è¯æå–
            simple_keywords = [
                word.strip()
                for word in potential_title.lower().split()
                if len(word.strip()) > 3
            ][:5]

            # ğŸ”‘ 3. æå–sectionsï¼ˆä»æ‰€æœ‰æ–‡ä»¶ä¸­æå–ï¼Œä½†æ¯ä¸ªsectionåªä¿ç•™å‰200å­—ï¼‰
            all_sections = []

            for idx, content in enumerate(file_contents):
                file_name = (
                    file_names[idx] if idx < len(file_names) else f"File {idx+1}"
                )
                lines = content.strip().split("\n")

                # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºsections
                current_section_lines = []

                for line in lines[:50]:  # æ¯ä¸ªæ–‡ä»¶åªçœ‹å‰50è¡Œ
                    line_stripped = line.strip()
                    if not line_stripped:
                        continue

                    # åˆ¤æ–­æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
                    if len(line_stripped) < 100 and (
                        line_stripped[0].isdigit()
                        or line_stripped.isupper()
                        or any(
                            marker in line_stripped.lower()
                            for marker in ["step", "phase", "stage", "chapter"]
                        )
                    ):
                        if current_section_lines:
                            content_preview = " ".join(current_section_lines)[:200]
                            all_sections.append(
                                {
                                    "title": f"{file_name}: {current_section_lines[0][:100]}",
                                    "content": content_preview,  # âœ… åªä¿ç•™å‰200å­—
                                    "level": 1,
                                    "source_file": file_name,
                                }
                            )
                            current_section_lines = [line_stripped]
                        else:
                            current_section_lines = [line_stripped]
                    else:
                        if len(current_section_lines) < 3:
                            current_section_lines.append(line_stripped)

                # ä¿å­˜æœ€åä¸€ä¸ªsection
                if current_section_lines:
                    content_preview = " ".join(current_section_lines)[:200]
                    all_sections.append(
                        {
                            "title": f"{file_name}: {current_section_lines[0][:100]}",
                            "content": content_preview,
                            "level": 1,
                            "source_file": file_name,
                        }
                    )

            # å¦‚æœæ²¡æœ‰æå–åˆ°sectionsï¼Œä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºä¸€ä¸ªç®€å•section
            if not all_sections:
                all_sections = [
                    {
                        "title": file_names[i]
                        if i < len(file_names)
                        else f"File {i+1}",
                        "content": content[:200] + "...",  # âœ… åªä¿ç•™å‰200å­—
                        "level": 1,
                        "source_file": file_names[i]
                        if i < len(file_names)
                        else f"File {i+1}",
                    }
                    for i, content in enumerate(file_contents)
                ]

            # âœ… 4. åˆ›å»ºä¼˜åŒ–çš„ metadata
            merged_metadata = {
                "doc_id": f"doc-{generate(size=12)}",
                "title": potential_title,  # âœ… çœŸå®æ ‡é¢˜
                "subject": potential_title,
                "language": "en",
                "bypass_local_llm": True,
                # âœ… å…³é”®å­—æ®µ
                "keywords": simple_keywords,
                # âœ… sectionsï¼šåªåŒ…å«ç« èŠ‚æ ‡é¢˜å’Œå‰200å­—é¢„è§ˆ
                "sections": all_sections[:15],  # æœ€å¤š15ä¸ªsections
                # âœ… facets
                "facets": {
                    "main_topic": {
                        "summary": potential_title,
                        "items": [
                            {
                                "value": kw,
                                "evidence": "",
                                "location": "",
                                "confidence": 0.8,
                            }
                            for kw in simple_keywords
                        ],
                    },
                    "source_files": {
                        "summary": f"Content from {len(file_contents)} file(s)",
                        "items": [
                            {
                                "value": name,
                                "evidence": "",
                                "location": "",
                                "confidence": 1.0,
                            }
                            for name in file_names
                        ],
                    },
                },
                # âœ… key_values
                "key_values": [
                    {"key": "document_title", "value": potential_title},
                    {"key": "file_count", "value": str(len(file_contents))},
                    {"key": "processing_mode", "value": "direct"},
                    {"key": "source_files", "value": ", ".join(file_names[:3])},
                ],
                # âœ… tags
                "tags": simple_keywords,
                # å…¶ä»–å¿…éœ€å­—æ®µ
                "source_count": len(file_contents),
                "source_files": file_names,
                "triples": [],
                "questions": [],
                "risks": [],
                "actions_todo": [],
                "metrics": [],
                "tables": [],
                "figures": [],
                "extra": {
                    "processing_mode": "direct",
                    "note": "Extracted structure without full text to reduce prompt size",
                    "file_names": file_names,
                    "total_length": sum(len(c) for c in file_contents),
                    "truncated": True,
                },
            }

            # âŒ å…³é”®ï¼šä¸æ·»åŠ  raw_textã€full_content æˆ–å®Œæ•´çš„ combined_textï¼

            framework_result = process_with_global_llm(
                metadata=merged_metadata, model=model, use_mock=False
            )
            print("âœ… Global LLM completed")

        # âœ… MODIFIED: æ”¯æŒå¤š POV è¾“å‡º
        frameworks = framework_result.get("frameworks", [framework_result])

        # ğŸ”¥ æ­¥éª¤ 3: ç”Ÿæˆ framework IDs(å‰ç«¯ä¼šä¿å­˜åˆ° Firebase)
        print(
            "ğŸ’¾ Step 3: Generating framework IDs (data will be saved to Firebase by frontend)..."
        )

        saved_ids = []
        for fw_data in frameworks:
            # ä¸ºæ¯ä¸ª framework ç”Ÿæˆå”¯ä¸€ ID
            fw_id = f"fw_{generate(size=12)}"
            fw_data["id"] = fw_id  # æ·»åŠ  ID åˆ° framework æ•°æ®ä¸­
            saved_ids.append(fw_id)

        print(f"âœ… Generated {len(saved_ids)} framework IDs: {saved_ids}")

        # âœ… MODIFIED: åŒæ—¶è¿”å›å•ä¸ªä¸å¤šä¸ª
        return GenerateResponse(
            success=True,
            framework_id=saved_ids[0] if saved_ids else None,
            framework=frameworks[0] if frameworks else None,
            frameworks=frameworks,
            metadata=merged_metadata,
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        import traceback

        traceback.print_exc()
        return GenerateResponse(success=False, error=str(e))
    finally:
        # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
        for temp_path in temp_paths:
            if os.path.exists(temp_path):
                try:
                    os.unlink(temp_path)
                except:
                    pass


# æ–°å¢ï¼šè·å–å½“å‰ç”¨æˆ·çš„æ‰€æœ‰ frameworks
@router.get("/my-frameworks", response_model=List[FrameworkListResponse])
def get_my_frameworks(user_id: str = Query(None), db: Session = Depends(get_db)):
    """
    è·å–å½“å‰ç”¨æˆ·åˆ›å»ºçš„æ‰€æœ‰ frameworks

    æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
    ç”¨äº "Your Frameworks" åˆ—è¡¨é¡µ
    """

    frameworks = (
        db.query(Framework)
        .filter(Framework.creator_id == user_id)
        .order_by(Framework.created_at.desc())
        .all()
    )

    result = []
    for fw in frameworks:
        # è§£æ artefacts ç”¨äºé¢„è§ˆ
        artefacts = json.loads(fw.artefacts_json)
        additional = artefacts.get("additional", [])

        # åªå–å‰3ä¸ª artefact ç”¨äºå¡ç‰‡æ˜¾ç¤º
        preview_artefacts = []
        if additional:
            for art in additional[:3]:
                preview_artefacts.append(
                    {
                        "name": art.get("name", ""),
                        "description": art.get("description", "")[:100],  # æˆªæ–­æè¿°
                    }
                )

        result.append(
            FrameworkListResponse(
                id=fw.id,
                title=fw.title,
                version=fw.version,
                family=fw.family,
                confidence=fw.confidence,
                created_at=fw.created_at,
                updated_at=fw.updated_at,
                preview_artefacts=preview_artefacts,
            )
        )

    return result


# æ–°å¢ï¼šæŒ‰ family åˆ†ç»„è·å– frameworks
@router.get("/my-frameworks/by-family")
def get_my_frameworks_by_family(
    user_id: str = Depends(get_current_user_id), db: Session = Depends(get_db)
):
    """
    è·å–å½“å‰ç”¨æˆ·çš„ frameworksï¼ŒæŒ‰ family åˆ†ç»„

    è¿”å›æ ¼å¼:
    {
        "Financial": [framework1, framework2, ...],
        "Healthcare": [...],
        ...
    }
    """

    frameworks = (
        db.query(Framework)
        .filter(Framework.creator_id == user_id)
        .order_by(Framework.created_at.desc())
        .all()
    )

    # æŒ‰ family åˆ†ç»„
    grouped = {}
    for fw in frameworks:
        family = fw.family or "Other"

        if family not in grouped:
            grouped[family] = []

        # è§£æ artefacts
        artefacts = json.loads(fw.artefacts_json)
        additional = artefacts.get("additional", [])

        preview_artefacts = []
        if additional:
            for art in additional[:3]:
                preview_artefacts.append(
                    {
                        "name": art.get("name", ""),
                        "description": art.get("description", "")[:100],
                    }
                )

        grouped[family].append(
            {
                "id": fw.id,
                "title": fw.title,
                "version": fw.version,
                "family": fw.family,
                "confidence": fw.confidence,
                "created_at": fw.created_at.isoformat(),
                "updated_at": fw.updated_at.isoformat(),
                "preview_artefacts": preview_artefacts,
            }
        )

    return grouped


# æ–°å¢ï¼šè·å–å•ä¸ª framework çš„è¯¦ç»†ä¿¡æ¯
@router.get("/{framework_id}", response_model=FrameworkDetailResponse)
def get_framework_detail(
    framework_id: str,
    user_id: str = Depends(get_current_user_id),
    db: Session = Depends(get_db),
):
    """
    è·å– framework çš„å®Œæ•´ä¿¡æ¯

    åªèƒ½è®¿é—®è‡ªå·±åˆ›å»ºçš„ framework
    """

    framework = (
        db.query(Framework)
        .filter(
            Framework.id == framework_id, Framework.creator_id == user_id  # ç¡®ä¿åªèƒ½è®¿é—®è‡ªå·±çš„
        )
        .first()
    )

    if not framework:
        raise HTTPException(
            status_code=404,
            detail="Framework not found or you don't have permission to access it",
        )

    return FrameworkDetailResponse(
        id=framework.id,
        title=framework.title,
        version=framework.version,
        family=framework.family,
        confidence=framework.confidence,
        creator_id=framework.creator_id,
        metadata=json.loads(framework.metadata_json),
        steps=json.loads(framework.steps_json),
        artefacts=json.loads(framework.artefacts_json),
        risks=json.loads(framework.risks_json),
        escalation=json.loads(framework.escalation_json),
        created_at=framework.created_at,
        updated_at=framework.updated_at,
    )


# æ–°å¢ï¼šç»‘å®šä¿¡æ¯æ¥å£ (/api/frameworks/{id}/binding)
@router.get("/{framework_id}/binding")
def get_framework_binding(
    framework_id: str,
    user_id: str = Depends(get_current_user_id),
    db: Session = Depends(get_db),
):
    """
    è·å– framework çš„ POVã€familyã€confidence ç»‘å®šä¿¡æ¯
    ï¼ˆç”¨äºå‰ç«¯åœ¨æ¡†æ¶å¡ç‰‡æˆ–è¯¦æƒ…é¡µä¸­åŒæ—¶æ˜¾ç¤º POV ä¸ä¿¡å¿ƒåº¦ï¼‰
    """
    # ğŸ” æŸ¥è¯¢å½“å‰ç”¨æˆ·çš„ framework
    fw = (
        db.query(Framework)
        .filter(Framework.id == framework_id, Framework.creator_id == user_id)
        .first()
    )

    if not fw:
        raise HTTPException(
            status_code=404, detail="Framework not found or access denied"
        )

    # å°è¯•ä» raw_framework_json ä¸­æå– POVï¼ˆAI è¿”å›çš„å®Œæ•´å†…å®¹ï¼‰
    pov_value = None
    try:
        if fw.raw_framework_json:
            raw_data = json.loads(fw.raw_framework_json)
            pov_value = raw_data.get("pov")
    except Exception:
        pov_value = None

    # âœ… è¿”å›ç»Ÿä¸€ç»‘å®šä¿¡æ¯
    return {
        "id": fw.id,
        "title": fw.title,
        "pov": pov_value,
        "family": fw.family,
        "confidence": fw.confidence,
        "created_at": fw.created_at,
        "updated_at": fw.updated_at,
    }


# æ–°å¢ï¼šæ›´æ–° framework
@router.put("/{framework_id}")
def update_framework(
    framework_id: str,
    framework_data: dict,
    user_id: str = Depends(get_current_user_id),
    db: Session = Depends(get_db),
):
    """
    æ›´æ–° frameworkï¼ˆä» Editor ä¿å­˜ï¼‰

    åªèƒ½æ›´æ–°è‡ªå·±åˆ›å»ºçš„ framework
    """

    framework = (
        db.query(Framework)
        .filter(Framework.id == framework_id, Framework.creator_id == user_id)
        .first()
    )

    if not framework:
        raise HTTPException(
            status_code=404, detail="Framework not found or you don't have permission"
        )

    # æ›´æ–°å­—æ®µ
    metadata = framework_data.get("metadata", {})
    framework.title = metadata.get("title", framework.title)
    framework.version = metadata.get("version", framework.version)

    # æ›´æ–° JSON å­—æ®µ
    framework.metadata_json = json.dumps(metadata, ensure_ascii=False)
    framework.steps_json = json.dumps(
        framework_data.get("steps", []), ensure_ascii=False
    )
    framework.artefacts_json = json.dumps(
        framework_data.get("artefacts", {}), ensure_ascii=False
    )
    framework.risks_json = json.dumps(
        framework_data.get("risks", []), ensure_ascii=False
    )
    framework.escalation_json = json.dumps(
        framework_data.get("escalation", []), ensure_ascii=False
    )

    framework.updated_at = datetime.utcnow()

    db.commit()
    db.refresh(framework)

    return {
        "success": True,
        "message": "Framework updated successfully",
        "framework_id": framework.id,
    }


# æ–°å¢ï¼šåˆ é™¤ framework
@router.delete("/{framework_id}")
def delete_framework(
    framework_id: str,
    user_id: str = Depends(get_current_user_id),
    db: Session = Depends(get_db),
):
    """
    åˆ é™¤ framework

    åªèƒ½åˆ é™¤è‡ªå·±åˆ›å»ºçš„ framework
    """

    framework = (
        db.query(Framework)
        .filter(Framework.id == framework_id, Framework.creator_id == user_id)
        .first()
    )

    if not framework:
        raise HTTPException(
            status_code=404, detail="Framework not found or you don't have permission"
        )

    db.delete(framework)
    db.commit()

    return {"success": True, "message": "Framework deleted successfully"}


# Export endpoint
@router.post("/export-markdown")
async def export_markdown_from_data(framework_data: dict):
    """
    æ¥æ”¶å®Œæ•´çš„æ¡†æ¶æ•°æ®ï¼Œç”Ÿæˆå¹¶è¿”å› Markdown æ–‡ä»¶

    Request Body:
    {
      "id": "framework-xxx",
      "metadata": {...},
      "steps": [...],
      "artefacts": {...},
      "risks": [...],
      "escalation": [...]
    }
    """
    try:
        # ç”Ÿæˆ Markdown å†…å®¹
        markdown_content = generate_markdown(framework_data)

        # ç”Ÿæˆæ–‡ä»¶å
        title = framework_data.get("metadata", {}).get("title", "framework")
        # æ¸…ç†æ–‡ä»¶åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
        safe_title = "".join(
            c if c.isalnum() or c in (" ", "-", "_") else "_" for c in title
        )
        filename = f"{safe_title.replace(' ', '_')}.md"

        # è¿”å›æ–‡ä»¶
        return Response(
            content=markdown_content,
            media_type="text/markdown",
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )

    except Exception as e:
        import traceback

        print("âŒ Export Error:")
        print(traceback.format_exc())

        raise HTTPException(
            status_code=500, detail=f"Failed to export markdown: {str(e)}"
        )


@router.post("/regenerate")
async def regenerate_framework(request: RegenerateRequest):
    """
    é‡æ–°ç”Ÿæˆæ¡†æ¶ï¼ˆç”¨æˆ·ç¼–è¾‘åçš„æ”¹è¿›ï¼‰

    ç”¨æˆ·å¯ä»¥é€‰æ‹©ï¼š
    1. Cloud Processing (OpenAI) - å¿«é€Ÿã€é«˜è´¨é‡ã€ä¿ç•™æ‰€æœ‰ç”¨æˆ·ç¼–è¾‘
    2. Local Processing (Ollama) - éšç§ä¼˜å…ˆã€å¯èƒ½ä¸¢å¤±ç»†èŠ‚
    """
    try:
        if request.use_local:
            # ========== æœ¬åœ°å¤„ç†æ¨¡å¼ ==========
            print("ğŸ”’ Using Local Processing (Ollama)")

            # æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
            import requests

            try:
                requests.get("http://127.0.0.1:11434", timeout=2)
            except requests.exceptions.RequestException:
                raise HTTPException(
                    status_code=503,
                    detail="Ollama is not running. Please start Ollama: 'ollama serve'",
                )

            # æ­¥éª¤ 1: å°†ç”¨æˆ·ç¼–è¾‘çš„æ¡†æ¶è½¬æ¢å›æ–‡æœ¬ï¼ˆæ¨¡æ‹Ÿ reverse engineeringï¼‰
            framework_text = convert_framework_to_text(request.framework)

            # æ­¥éª¤ 2: Local LLM é‡æ–°æå– metadata
            from llm_local import extract_seed_from_text, OllamaClient

            llm = OllamaClient(model="llama3.1:8b", host="http://127.0.0.1:11434")
            metadata = extract_seed_from_text(framework_text, llm=llm)

            # æ­¥éª¤ 3: Global LLM ç”Ÿæˆæ–°æ¡†æ¶ï¼ˆæˆ–ä½¿ç”¨ mockï¼‰
            api_key, base_url = resolve_api_settings(None, None)
            if api_key:
                from llm_global import call_openai_framework

                improved_framework = call_openai_framework(
                    md=metadata,
                    model="gpt-4o",
                    timeout=180,
                    api_key=api_key,
                    base_url=base_url,
                    verbose=True,
                )
            else:
                from llm_global import build_mock_framework

                improved_framework = build_mock_framework(metadata)

            return {
                "success": True,
                "framework": improved_framework,
                "method": "local",
                "message": "Framework regenerated using local processing",
            }

        else:
            # ========== äº‘ç«¯å¤„ç†æ¨¡å¼ï¼ˆæ¨èï¼‰==========
            print("â˜ï¸ Using Cloud Processing (OpenAI)")

            # æ£€æŸ¥ API key
            api_key, base_url = resolve_api_settings(None, None)
            if not api_key:
                raise HTTPException(
                    status_code=400,
                    detail="OpenAI API key not configured. Please use local processing instead.",
                )

            # ç›´æ¥å‘é€å®Œæ•´æ¡†æ¶ç»™ OpenAI è¿›è¡Œæ”¹è¿›
            from openai import OpenAI
            import httpx
            import os

            # æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡
            original_proxies = {}
            proxy_keys = [
                "HTTP_PROXY",
                "HTTPS_PROXY",
                "http_proxy",
                "https_proxy",
                "ALL_PROXY",
                "all_proxy",
                "NO_PROXY",
                "no_proxy",
            ]

            for key in proxy_keys:
                if key in os.environ:
                    original_proxies[key] = os.environ[key]
                    del os.environ[key]
            try:
                if base_url:
                    client = OpenAI(
                        api_key=api_key,
                        base_url=base_url,
                        timeout=timeout,
                        max_retries=2,
                    )
                else:
                    client = OpenAI(api_key=api_key, timeout=timeout, max_retries=2)

                # æ„å»º prompt
                system_prompt = (
                    "You are a framework improvement assistant. "
                    "The user has edited a framework and wants you to review and improve it. "
                    "CRITICAL: Keep ALL user modifications intact. Only fill in missing parts and suggest improvements. "
                    "Return the improved framework as valid JSON matching the original structure."
                )

                user_prompt = (
                    "Here is a framework that the user has edited:\n\n"
                    f"{json.dumps(request.framework, indent=2)}\n\n"
                    "Please:\n"
                    "1. **Keep all user modifications intact** (especially steps, risks, escalation)\n"
                    "2. Fill in missing sections if any:\n"
                    "   - Add 'trigger_context' or 'pov' if missing\n"
                    "   - Add 'inputs_required' if missing\n"
                    "   - Add 'research_required' if missing\n"
                    "   - Add 'attribution' if appropriate\n"
                    "   - Add 'quadrant' (QI/QII/QIII/QIV) if appropriate\n"
                    "3. Ensure consistency across all sections\n"
                    "4. Improve descriptions to be more specific and actionable\n"
                    "5. Return the complete improved framework as JSON\n\n"
                    "IMPORTANT: Do NOT remove or significantly change user's content. Only enhance and complete."
                )

                print("ğŸ“¤ Sending request to OpenAI...")
                response = client.chat.completions.create(
                    model="gpt-4o",
                    temperature=0.3,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                )

                result_text = response.choices[0].message.content.strip()
                print("ğŸ“¥ Received response from OpenAI")

                # è§£æ JSON
                from llm_global import robust_json_loads

                improved_framework = robust_json_loads(result_text)

                return {
                    "success": True,
                    "framework": improved_framework,
                    "method": "cloud",
                    "message": "Framework regenerated using cloud processing",
                }

            finally:
                # æ¢å¤ä»£ç†è®¾ç½®
                for key, value in original_proxies.items():
                    os.environ[key] = value

                # å…³é—­ HTTP client
                if "http_client" in locals():
                    try:
                        http_client.close()
                    except:
                        pass

    except HTTPException:
        raise
    except Exception as e:
        import traceback

        print("âŒ Regeneration Error:")
        print(traceback.format_exc())

        raise HTTPException(
            status_code=500, detail=f"Failed to regenerate framework: {str(e)}"
        )


def convert_framework_to_text(framework: dict) -> str:
    """
    å°†æ¡†æ¶ JSON è½¬æ¢å›æ–‡æœ¬ï¼ˆç”¨äºæœ¬åœ° LLM å¤„ç†ï¼‰
    è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œç”¨äºæ¨¡æ‹ŸåŸå§‹æ–‡æ¡£
    """
    parts = []

    # Title
    metadata = framework.get("metadata", {})
    title = metadata.get("title", "Framework")
    parts.append(f"# {title}\n")

    # Steps
    steps = framework.get("steps", [])
    if steps:
        parts.append("\n## Framework Steps\n")
        for step in steps:
            parts.append(f"\n### {step.get('name', 'Step')}")
            parts.append(step.get("description", ""))
            sub_steps = step.get("subSteps", [])
            if sub_steps:
                for sub in sub_steps:
                    parts.append(f"- {sub}")

    # Risks
    risks = framework.get("risks", [])
    if risks:
        parts.append("\n## Risks\n")
        for risk in risks:
            parts.append(f"\n### {risk.get('title', 'Risk')}")
            parts.append(risk.get("description", ""))

    # Escalation
    escalation = framework.get("escalation", [])
    if escalation:
        parts.append("\n## Escalation Points\n")
        for esc in escalation:
            parts.append(f"- When: {esc.get('trigger', 'Unknown')}")
            parts.append(f"  Action: {esc.get('action', 'Escalate')}")

    return "\n".join(parts)


# ============= AI Merge Endpoint =============


class AIMergeRequest(BaseModel):
    """AI åˆå¹¶è¯·æ±‚æ¨¡å‹"""

    frameworks: List[dict]  # ç”¨æˆ·é€‰ä¸­çš„å¤šä¸ª frameworks


@router.post("/ai-merge")
async def ai_merge_frameworks(request: AIMergeRequest):
    """
    ä½¿ç”¨ AI æ™ºèƒ½åˆå¹¶å¤šä¸ª frameworks

    ä¸´æ—¶ç‰ˆæœ¬ï¼šä¸éœ€è¦è®¤è¯ï¼Œç›´æ¥è¿”å›æµ‹è¯•ç»“æœ
    """
    try:
        # éªŒè¯è¾“å…¥
        if not request.frameworks or len(request.frameworks) < 2:
            raise HTTPException(
                status_code=400, detail="Please select at least 2 frameworks to merge"
            )

        if len(request.frameworks) > 10:
            raise HTTPException(
                status_code=400, detail="Cannot merge more than 10 frameworks at once"
            )

        print(f"ğŸ”€ AI Merge: Merging {len(request.frameworks)} frameworks")

        # æ£€æŸ¥ API key
        api_key, base_url = resolve_api_settings(None, None)
        if not api_key:
            # å¦‚æœæ²¡æœ‰ API keyï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„åˆå¹¶ç»“æœ
            print("âš ï¸ No API key, returning mock merge")
            return {
                "success": True,
                "merged_framework": {
                    "name": "Merged Framework (Mock)",
                    "description": "This is a test merge of "
                    + str(len(request.frameworks))
                    + " frameworks.",
                    "subSteps": [
                        "Combined step 1",
                        "Combined step 2",
                        "Combined step 3",
                    ],
                },
            }

        # å‡†å¤‡åˆå¹¶ prompt
        frameworks_text = []
        for i, fw in enumerate(request.frameworks, 1):
            frameworks_text.append(f"\n{'='*60}")
            frameworks_text.append(f"FRAMEWORK {i}: {fw.get('name', 'Unnamed')}")
            frameworks_text.append(f"{'='*60}\n")

            # Description
            if fw.get("description"):
                frameworks_text.append(f"Description:\n{fw['description']}\n")

            # Sub-steps
            if fw.get("subSteps"):
                frameworks_text.append("Sub-steps:")
                for j, step in enumerate(fw["subSteps"], 1):
                    frameworks_text.append(f"  {j}. {step}")
                frameworks_text.append("")

        combined_text = "\n".join(frameworks_text)

        # è°ƒç”¨ OpenAI
        from openai import OpenAI
        import httpx
        import os

        # æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡
        original_proxies = {}
        proxy_keys = [
            "HTTP_PROXY",
            "HTTPS_PROXY",
            "http_proxy",
            "https_proxy",
            "ALL_PROXY",
            "all_proxy",
            "NO_PROXY",
            "no_proxy",
        ]

        for key in proxy_keys:
            if key in os.environ:
                original_proxies[key] = os.environ[key]
                del os.environ[key]

        try:
            # OpenAI 2.6.1 è‡ªåŠ¨å¤„ç†é‡è¯•å’Œè¶…æ—¶
            if base_url:
                client = OpenAI(
                    api_key=api_key,
                    base_url=base_url,
                    timeout=300.0,  # 5åˆ†é’Ÿè¶…æ—¶
                    max_retries=2,
                )
            else:
                client = OpenAI(api_key=api_key, timeout=300.0, max_retries=2)

            # æ„å»º prompt
            system_prompt = (
                "You are a framework merging assistant. "
                "Your task is to intelligently combine multiple frameworks into one cohesive framework. "
                "You should:\n"
                "1. Identify common themes and consolidate similar content\n"
                "2. Remove redundancy while preserving unique insights from each framework\n"
                "3. Organize the merged content logically\n"
                "4. Create a clear, comprehensive description that captures all key aspects\n"
                "5. Combine sub-steps in a logical order\n"
                "6. Generate an appropriate name for the merged framework\n\n"
                "Return ONLY a valid JSON object with this structure:\n"
                "{\n"
                '  "name": "Merged Framework Name",\n'
                '  "description": "Comprehensive description...",\n'
                '  "subSteps": ["Step 1", "Step 2", ...]\n'
                "}"
            )

            user_prompt = (
                f"Please merge these {len(request.frameworks)} frameworks into one:\n\n"
                f"{combined_text}\n\n"
                "Create a new framework that:\n"
                "- Captures the essence of all input frameworks\n"
                "- Eliminates redundancy and contradictions\n"
                "- Provides a clear, actionable structure\n"
                "- Has a descriptive name that reflects the merged content\n\n"
                "Return the merged framework as JSON."
            )

            print("ğŸ“¤ Sending merge request to OpenAI...")
            response = client.chat.completions.create(
                model="gpt-4o",
                temperature=0.4,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
            )

            result_text = response.choices[0].message.content.strip()
            print("ğŸ“¥ Received response from OpenAI")

            # è§£æ JSON
            from llm_global import robust_json_loads

            merged_framework = robust_json_loads(result_text)

            # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
            if not merged_framework.get("name"):
                merged_framework["name"] = "AI Merged Framework"

            if not merged_framework.get("description"):
                merged_framework["description"] = ""

            if not merged_framework.get("subSteps"):
                merged_framework["subSteps"] = []

            print(f"âœ… Successfully merged into: {merged_framework['name']}")

            return {"success": True, "merged_framework": merged_framework}

        finally:
            # æ¢å¤ä»£ç†è®¾ç½®
            for key, value in original_proxies.items():
                os.environ[key] = value

            # å…³é—­ HTTP client
            if "http_client" in locals():
                try:
                    http_client.close()
                except:
                    pass

    except HTTPException:
        raise
    except Exception as e:
        import traceback

        print("âŒ AI Merge Error:")
        print(traceback.format_exc())

        return {"success": False, "error": str(e)}
